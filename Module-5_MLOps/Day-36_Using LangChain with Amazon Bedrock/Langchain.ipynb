{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea99b6-0d9c-49f7-92cd-a3b87603443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain_huggingface langchain_chroma chromadb sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa591b-604a-42bc-88e3-54c984db8942",
   "metadata": {},
   "source": [
    "### Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a70e9-ec51-455a-b836-5ea8ac192351",
   "metadata": {},
   "source": [
    "#### Without Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2d86f3-86c3-4ffa-a6aa-8498ff2e3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e921b00d-2fe0-4a24-ad05-120f71e9bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "llm = ChatBedrockConverse(\n",
    "    model = model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8935b7be-92ce-4ef7-ba53-915139d45901",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Hi, I am Saqlain.\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "body = json.dumps({\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": messages,\n",
    "    \"inferenceConfig\": {\n",
    "        \"maxTokens\": 500,\n",
    "        \"temperature\": 0.7,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4c9fb2-d38b-4eba-980e-b6848cabf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0435c3d-3215-4d52-b597-7de41adae7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Saqlain! It's nice to meet you. How can I assist you today? If you have any questions or need help with something, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d85ad5-6c23-4eb0-a972-0ee375099169",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Can you tell my name?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "body = json.dumps({\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": messages,\n",
    "    \"inferenceConfig\": {\n",
    "        \"maxTokens\": 500,\n",
    "        \"temperature\": 0.7,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e485b183-6064-41d1-95de-a0d2220cbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64e382b-134d-41c0-a512-db67a356269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't tell you your name as it's personal information that I can't access or share. If you have any other questions or need assistance with something else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa0e1d-034c-477d-b121-ada343be3163",
   "metadata": {},
   "source": [
    "#### With Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4038feb6-dfa4-4222-a6f0-d2fd644a75d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello Saqlain! It's nice to meet you. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "user_input_1 = HumanMessage(content=\"Hi, I am Saqlain.\")\n",
    "chat_history.append(user_input_1)\n",
    "\n",
    "response_1 = llm.invoke(chat_history)\n",
    "print(\"AI:\", response_1.content)\n",
    "chat_history.append(response_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6646f32c-f77c-4ed8-a18f-0fef6a1f7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Your name is Saqlain. If you have any questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_input_2 = HumanMessage(content=\"What is my name?\")\n",
    "chat_history.append(user_input_2)\n",
    "\n",
    "response_2 = llm.invoke(chat_history)\n",
    "print(\"AI:\", response_2.content)\n",
    "chat_history.append(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6faa29-561a-4429-9ac9-01ab55b09138",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3822e3aa-6552-45eb-ba26-dc4e421678a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to plain input:\n",
      "Hello Saqlain! It's nice to meet you. If there's anything you'd like to discuss or any questions you have, feel free to let me know. Whether it's about a specific topic, a problem you're facing, or just a general inquiry, I'm here to help!\n",
      "\n",
      "Response to structured message:\n",
      "Hello Saqlain! It's nice to meet you. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the model\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "llm = ChatBedrockConverse(model=model_id)\n",
    "\n",
    "# --- 1. Using a plain string input ---\n",
    "plain_input = \"Hi, I am Saqlain.\"\n",
    "response_plain = llm.invoke(plain_input)\n",
    "print(\"Response to plain input:\")\n",
    "print(response_plain.content)\n",
    "\n",
    "# --- 2. Using structured messages ---\n",
    "structured_message = HumanMessage(content=\"Hi, I am Saqlain.\")\n",
    "response_structured = llm.invoke([structured_message])\n",
    "print(\"\\nResponse to structured message:\")\n",
    "print(response_structured.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b79b3db0-b4f0-4965-99fd-d3cd72a01e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi, I am Saqlain.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(structured_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20413b-4af9-4c50-85b8-4deb49d4fd65",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed16a7-1d06-4e03-bbcc-c299076c8ee2",
   "metadata": {},
   "source": [
    "#### Without Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "336f11a2-0383-4cbc-9ad5-363fe2e34ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my most recent update, the President of Pakistan is Arif Alvi. However, it's important to note that political positions can change, and for the most current information, you should consult reliable and up-to-date sources such as official government websites or reputable news outlets.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "llm = ChatBedrockConverse(model=model_id)\n",
    "\n",
    "# User input\n",
    "user_input = \"Who is President of Pakistan?\"\n",
    "\n",
    "# Generate response\n",
    "response = llm.invoke([HumanMessage(content=user_input)])\n",
    "\n",
    "# Output the response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cbc79d6f-575b-4b3b-a856-e452d18f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# Your in-memory data\n",
    "texts = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The largest planet in our solar system is Jupiter.\",\n",
    "    \"The Great Wall of China is visible from space.\",\n",
    "    \"President of Pakistan is Asif Ali Zardari.\"\n",
    "]\n",
    "\n",
    "# Optional metadata for each document\n",
    "metadatas = [\n",
    "    {\"source\": \"fact1\"},\n",
    "    {\"source\": \"fact2\"},\n",
    "    {\"source\": \"fact3\"},\n",
    "    {\"source\": \"fact4\"}\n",
    "]\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "# Initialize Chroma vector store with in-memory data\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=\"./vectorstore\"  # Optional: specify if you want persistence\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2bf288e-3cbe-4953-8021-785bd2bacf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's query\n",
    "query = \"Who is President of Pakistan?\"\n",
    "\n",
    "# Retrieve top 2 relevant documents\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "# Extract the content of the retrieved documents\n",
    "retrieved_chunks = [doc.page_content for doc in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31fe5fff-44e5-4e09-8258-c5de592eb252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: Who is president of Pakistan?\\nAnswer:',\n",
       " 'President of Pakistan is Asif Ali Zardari.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c9efd6f-c550-4b09-ac95-ff9c0a6fd445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answr only based on given context below Context:\n",
      "Question: Who is president of Pakistan?\n",
      "Answer:\n",
      "President of Pakistan is Asif Ali Zardari.\n",
      "\n",
      "Question: Who is President of Pakistan?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "llm = ChatBedrockConverse(model=model_id)\n",
    "\n",
    "# Construct the prompt with retrieved context\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "prompt = f\"Answer only based on given context below Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "78d80ba0-5408-414d-b580-71f02ec1a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President of Pakistan is Asif Ali Zardari.\n",
      "\n",
      "(Note: The context provided only includes information about Asif Ali Zardari being the President of Pakistan. If the context changes or if new information is available, the answer might need to be updated accordingly.)\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "# Output the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c80da4-ddda-4c7d-830c-3a6eb74dccc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04687c8a-16e9-4692-a2f1-ecc325bc339d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "524a1538-4c1d-429d-9749-a9e11967f038",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4eae4f-5c5b-47da-b2a4-a8e5de24c8f6",
   "metadata": {},
   "source": [
    "#### Without Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "69705690-a747-4427-ac8d-3f8a3f195d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatBedrockConverse(model=\"amazon.nova-lite-v1:0\")\n",
    "\n",
    "# Craft a prompt that instructs the model to respond in JSON format\n",
    "prompt = (\n",
    "    \"Tell me a joke about cats. \"\n",
    "    \"Respond in JSON format with the following fields:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"setup\": \"string\",\\n'\n",
    "    '  \"punchline\": \"string\"\\n'\n",
    "    \"}\"\n",
    ")\n",
    "\n",
    "# Invoke the model with the crafted prompt\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "# Extract and parse the content\n",
    "content = response.content\n",
    "joke = json.loads(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec420e-8dda-43be-9596-64d7922a86ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b646116-1ad9-4bd0-86d5-2c3b0920a23f",
   "metadata": {},
   "source": [
    "#### With Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aa59087e-0799-4ca5-9205-21f35f904e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "30bfcd16-93db-440a-8a59-0a9bb7fe0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Tell me a joke about {topic}. \"\n",
    "    \"Respond in JSON format with the following fields:\\n\"\n",
    "    \"{{\\n\"\n",
    "    '  \"setup\": \"string\",\\n'\n",
    "    '  \"punchline\": \"string\"\\n'\n",
    "    \"}}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ec5cadd2-367c-4db5-9e56-0732af18b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatBedrockConverse(model=\"amazon.nova-lite-v1:0\")\n",
    "\n",
    "# Invoke the model\n",
    "model_input = prompt.format(topic = 'cats') \n",
    "response = llm.invoke([HumanMessage(content=model_input)])\n",
    "\n",
    "# Extract and parse the content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bc3cc734-ff23-4465-a4aa-074b9e37bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the cat join the Red Cross?' punchline='Because it wanted to be a first-aid kit!'\n"
     ]
    }
   ],
   "source": [
    "content = response.content\n",
    "parsed_output = Joke(**json.loads(content))\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f131fe-3e87-49fb-bfd1-bdd8771bf59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f105aff-ecb1-4399-ae93-aa199cb9d72b",
   "metadata": {},
   "source": [
    "#### Langchain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b91eb211-ec52-4fbd-8fa2-249144994ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Why did the cat join the Red Cross?\n",
      "Punchline: Because she wanted to be a first-paw responder!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the Pydantic model for structured output\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n",
    "# Initialize the output parser with the Pydantic model\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Create the prompt template with format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a joke about {topic}.\\nRespond in JSON format with the following fields:\\n{{\\n 'setup': 'string',\\n 'punchline': 'string'\\n}}\",\n",
    "    input_variables=[\"topic\"],\n",
    ")\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatBedrockConverse(model=\"amazon.nova-lite-v1:0\", temperature=0)\n",
    "\n",
    "# Chain the components using the pipe operator\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Invoke the chain to get the structured joke\n",
    "joke = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "# Access and print the structured fields\n",
    "print(\"Setup:\", joke['setup'])\n",
    "print(\"Punchline:\", joke['punchline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a18d8-0eb2-472f-b35d-7de8ec2fba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b45f9-5e94-4307-ae76-7d87cadaabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5590c0dc-109d-4273-8499-efe45a349018",
   "metadata": {},
   "source": [
    "### Integrating Bedrock Knowledge Bases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3944fd1-8382-489b-9c4e-6ae7ed8ac099",
   "metadata": {},
   "source": [
    "- Create a separate User\n",
    "- Create an S3 bucket\n",
    "- Add PDF file in it\n",
    "- Create Knowledge base in Bedrock with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cdd73d87-8762-41a9-9914-3691b9e5d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=\"G8GBOPS6ZT\",\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 2}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "68ab25c2-1371-426e-a793-ff033e03aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"tell me about K2 mountain and Pakistan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d624c7df-620c-43a3-89a6-6eb0970145ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5df3db8d-a60c-45fa-adee-7b33ea2d0dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It has the nakedness of the world before the first man—or of the cindered planet after the last.[29]     André Weil named K3 surfaces in mathematics partly after the beauty of the mountain K2.[30]     K2 lies in the northwestern Karakoram Range. It is located in the Baltistan region of Gilgit–Baltistan, Pakistan, and the Taxkorgan Tajik Autonomous County of Xinjiang, China.[a] The Tarim sedimentary basin borders the range on the north and the Lesser Himalayas on the south. Melt waters from glaciers, such as those south and east of K2, feed agriculture in the valleys and contribute significantly to the regional fresh-water supply.     K2 is ranked 22nd by topographic prominence, a measure of a mountain's independent stature. It is a part of the same extended area of uplift (including the Karakoram, the Tibetan Plateau, and the Himalayas) as Mount Everest, and it is possible to follow a path from K2 to Everest that goes no lower than 4,594 metres (15,072 ft), at the Kora La on the Nepal/China border in the Mustang Lo. Many     The major peaks in Karakoram are rank identified     by height.     Legend： 1：K2, 2：Gasherbrum I, K5, 3：Broad Peak, 4：Gasherbrum II, K4\""
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a38c5f2f-369b-4291-8335-a2e948b58aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langgraph.graph.message import add_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b7352b5c-1afd-4df8-bcfe-af1100913820",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add_numbers(i : int, j, int) -> int:\n",
    "    \"\"\"Adds numbers provided in the query.\"\"\"\n",
    "    return i + j\n",
    "\n",
    "\n",
    "@tool\n",
    "def knowledge_retriever(query: str) -> str:\n",
    "    \"\"\"Retrieves information based on the query from Knowledge base related to Mountains in Pakistan.\"\"\"\n",
    "    knowledge = \"\"\n",
    "    for response in retriever.invoke(query):\n",
    "        knowledge += response.page_content + \"\\n\"\n",
    "    return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "646ea998-d377-4a5b-a537-884cdc513366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add_numbers, knowledge_retriever]\n",
    "\n",
    "agent_node = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2a6183b5-08ba-49b7-8da9-fb5121cd43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.set_finish_point(\"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "511f9dfa-2a59-4bcb-8de6-6a8888c4dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thinking>The error persists despite multiple attempts to call the 'add_numbers' tool correctly. The tool's validation seems to be expecting an 'int' parameter, which is not required based on the tool's description. Given the persistent error, I will inform the User about the issue and suggest they check the tool's documentation or contact support for further assistance.</thinking>\n",
      "\n",
      "I'm sorry, but there seems to be an issue with the 'add_numbers' tool. It's expecting an 'int' parameter, which is not required based on the tool's description. I recommend checking the tool's documentation or contacting support for further assistance. If you have any other questions or need assistance with a different tool, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = app.invoke({\"messages\": [HumanMessage(content=\"What is 45 plus 60?\")]})\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2c570207-5954-41cc-9e34-dc7b36511b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thinking>The knowledge_retriever tool has provided the information about the height of K2 Mountain. I can now directly answer the User's question.</thinking>\n",
      "\n",
      "The height of K2 Mountain is 8,611 meters (28,251 feet). It is the second-highest mountain on Earth, after Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = app.invoke({\"messages\": [HumanMessage(content=\"What is the height f K2 Mountain?\")]})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebfff6-0221-4320-9305-ec53fc009457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ee5a0-eafc-4548-b4a4-fe4c63e842d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
